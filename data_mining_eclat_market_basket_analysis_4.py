# -*- coding: utf-8 -*-
"""DATA MINING - ECLAT MARKET BASKET ANALYSIS 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X7wQ0ijzrm3kcV9TMPLI-NZltnZ81IJi

---
# I. Môi trường triển khai
---
"""

# Cài đặt thư viện dùng cho thuật toán ECLAT
# ------------------------------------------------------------------------------
! pip install pyECLAT

# Môi trường lập trình
# ------------------------------------------------------------------------------
from google.colab import drive
drive.mount("/content/gdrive")
folder = '/content/gdrive/MyDrive/Machine Learning'

# Các thư viện cơ bản
# ------------------------------------------------------------------------------
import pandas as pd
import numpy as np
import time
import math
import re
import itertools
import warnings
warnings.filterwarnings('ignore')

# Các thư viện khai phá luật kết hợp
# ------------------------------------------------------------------------------
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import association_rules
from mlxtend.frequent_patterns import apriori
from pyECLAT import ECLAT

# Visualization
# ------------------------------------------------------------------------------
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

"""---
# II. Mô tả dữ liệu
---

Bộ dữ liệu ghi lại mặt hàng trong giỏ hàng siêu thị của các khách hàng
*   Mỗi dòng đại diện cho một giao dịch
*   Mỗi cột đại diện cho một item trong giỏ hàng


"""

# Tải bộ dữ liệu "Market Basket Analysis 4.cvs" lên
# ------------------------------------------------------------------------------
df = pd.read_csv('/content/Market Basket Analysis 4.csv', header=None)

# Quan sát các dòng dữ liệu đầu
# ------------------------------------------------------------------------------
df.head()

# Quan sát các dòng dữ liệu cuối
# ------------------------------------------------------------------------------
df.tail()

# Kích thước bộ dữ liệu
# ------------------------------------------------------------------------------
df.shape

# Quan sát các cột dữ liệu
# ------------------------------------------------------------------------------
df.info()

"""# III. Phân tích khám phá dữ liệu (EDA)

## 1. Xử lý trùng trong giao dịch
"""

# Tính tổng số lượng từng sản phẩm
# ------------------------------------------------------------------------------
items_total = df.apply(pd.Series.value_counts).sum(axis=1)
items_total = pd.DataFrame({'items': items_total.index, 'transactions': items_total.values})
items_total.sort_values('transactions', ascending=False).head(15).reset_index(drop = True).style.background_gradient(cmap='Blues')

# Kiểm tra tính duy nhất của các item trong mỗi giao dịch
# ------------------------------------------------------------------------------
df['num_uniq'] = df.iloc[:,0:11].apply(pd.Series.nunique, axis=1) #Số item duy nhất trong giao dịch
df['num_item'] = df.iloc[:,0:11].count(axis=1) #Số item trong giao dịch
print("--- CÁC GIAO DỊCH CÓ CHỨA ITEM TRÙNG LẶP ---")
df[df['num_uniq'] != df['num_item']]

# Xử lý các giao dịch và đưa vào df mới sau đó kiểm tra lại tính duy nhất
# ------------------------------------------------------------------------------
df_cleaned = []

df = df.replace(np.nan, None)

for _, row in df.iloc[:,0:11].iterrows():
  temp = []
  for column, value in row.iteritems():
    if (value is not None) and (value not in temp):
      i = f"{value}"
      temp.append(i)
  df_cleaned.append(temp)

df_cleaned = pd.DataFrame(df_cleaned)
df_cleaned['num_uniq'] = df_cleaned.iloc[:,0:10].apply(pd.Series.nunique, axis=1)
df_cleaned['num_item'] = df_cleaned.iloc[:,0:10].count(axis=1)
print('Số giao dịch có chứa item trùng lặp là:', len(df_cleaned[df_cleaned['num_uniq'] != df_cleaned['num_item']]))

df_cleaned

# Thu được DataFrame sau khi xử lý (Vì lọc item trùng nên số cột cũng giảm)
# ------------------------------------------------------------------------------
df_cleaned = df_cleaned.iloc[:,0:10]
df_cleaned = df_cleaned.fillna(value= float('nan'))
df_cleaned

# Tính tổng số lượng từng sản phẩm sau khi xử lý
# ------------------------------------------------------------------------------
items_total_clean = df_cleaned.apply(pd.Series.value_counts).sum(axis=1)
items_total_clean = pd.DataFrame({'items': items_total_clean.index, 'transactions': items_total_clean.values})
items_total_clean.sort_values('transactions', ascending=False).head(15).reset_index(drop = True).style.background_gradient(cmap='Blues')

"""## 2. Phân tích sản phẩm"""

# Biểu đồ scarter plot thể hiện top 15 sản phẩm được xuất hiện trong giỏ hàng nhiều nhất
# ------------------------------------------------------------------------------
items_list = df_cleaned.values.flatten()
frequency = pd.Series(items_list).value_counts().head(15)
fig, ax = plt.subplots(figsize=(10, 6))
scatter = ax.scatter(frequency.values, frequency.index, s=frequency.values * 1.5, alpha=0.9, c=frequency.values, cmap='Blues')
ax.set_xlabel('Tần suất xuất hiện')
ax.set_ylabel('Sản phẩm')
ax.set_title('Top 15 sản phẩm xuất hiện nhiều nhất trong tập dữ liệu MBA4')
cbar = plt.colorbar(scatter)
cbar.ax.set_ylabel('Tần suất')
plt.show()

# Biểu đồ bar plot thể hiện tần suất của top 15 sản phẩm được xuất hiện trong giỏ hàng nhiều nhất
# ------------------------------------------------------------------------------
product_counts = df_cleaned.apply(pd.Series.value_counts)
total_samples = len(df)
product_frequencies = product_counts / total_samples
top_products = product_frequencies.sum(axis=1).sort_values(ascending=False).head(15)
plt.bar(top_products.index, top_products.values, color='#ABD0E6')
plt.xlabel('Sản phẩm')
plt.ylabel('Tần suất')
plt.title('Top 15 sản phẩm xuất hiện nhiều nhất')
plt.xticks(rotation=90)
plt.show()

# Tree Map biểu diễn số lượng top 50 mặt hàng xuất hiện nhiều nhất trong giỏ hàng
# ------------------------------------------------------------------------------
items_total_clean["all"] = "Tree Map"
items_total_clean = items_total_clean.sort_values('transactions', ascending=False)

fig = px.treemap(items_total_clean.head(50), path=['all', "items"], values='transactions',
                 color=items_total_clean["transactions"].head(50), hover_data=['items'],
                 color_continuous_scale='Blues',
                 title="Tree Map số lượng top 50 item xuất hiện nhiều nhất trong giỏ hàng")
fig.show()

"""## 3. Phân tích giao dịch"""

# Tính số item trong giao dịch và số lượng giao dịch
# ------------------------------------------------------------------------------
items_per_transaction = df_cleaned.count(axis=1)
items_per_transaction = items_per_transaction.value_counts()
items_per_transaction

# Biểu đồ bar plot thể hiện số lượng giao dịch ứng với mỗi số lượng item trong giao dịch
# ------------------------------------------------------------------------------
plt.bar(items_per_transaction.index, items_per_transaction.values, color='#ABD0E6')
plt.xlabel('Số lượng item')
plt.ylabel('Số lượng giao dịch')
plt.title('Số lượng item trong giao dịch')
plt.xticks(rotation=90)
plt.show()

"""---
# IV. Frequent Pattern Algorithms
---
Xét 3 thuật toán:
* Thuật toán ECLAT (thư viện pyECLAT)
* Thuật toán ECLAT (mô phỏng)
* Thuật toán Apriori
"""

# Nhập các ngưỡng tối thiểu khi khai phá dữ liệu
# ------------------------------------------------------------------------------
min_support = eval(input("Nhập độ hỗ trợ tối thiểu - minSup (e.g. 0.01): "))
min_confidence = eval(input("Nhập độ chắc chắn tối thiểu - minConf (e.g. 0.3): "))

"""## Thuật toán ECLAT (PyECLAT lib)"""

# Sử dụng hàm ECLAT thư viện pyECLAT để chuyển dạng dữ liệu thành bảng boolean
#   + Mỗi hàng thể hiện một giao dịch
#   + Mỗi cột thể hiện một mặt hàng trong toàn tập dữ liệu
# Ô sẽ mang giá trị 1 nếu mặt hàng đó có trong giao dịch và 0 nếu ngược lại
# ------------------------------------------------------------------------------
df1 = df_cleaned
eclat = ECLAT(data=df1)
eclat.df_bin

# Khai phá các tập phổ biến có sự kết hợp của 1 đến 2 item
# ------------------------------------------------------------------------------
start_time = time.time()

min_combination = 1
max_combination = 2
rule_indices, rule_supports = eclat.fit(min_support=min_support,
                                        min_combination = min_combination,
                                        max_combination = max_combination,
                                        separator = ', ',
                                        verbose = True)

elapsed_time_eclat1 = time.time() - start_time

# Dữ liệu dưới dạng Vertical được tạo bởi pyECLAT
# ------------------------------------------------------------------------------
rule_indices

# Chuyển các khóa của tập phổ biến thành frozenset và đứa vào một list itemsets
# ------------------------------------------------------------------------------
elapsed_time_eclat1 = elapsed_time_eclat1 - time.time()
itemsets = []
for i in list(rule_supports.keys()):
  i = i.split(', ')
  i = (frozenset(i))
  itemsets.append(i)

# Ta được tạo được một DataFrame gồm các tập phổ biến và support của chúng
# ------------------------------------------------------------------------------
freq_itemsets = pd.DataFrame({'support': list(rule_supports.values()),
                              'itemsets': itemsets})

# Sử dụng association_rules để khai phá các luật từ tập phổ biến tìm được
# ------------------------------------------------------------------------------
rule_ECLAT = association_rules(freq_itemsets, metric="confidence", min_threshold= min_confidence)
rule_ECLAT = rule_ECLAT[rule_ECLAT.lift > 1]
rule_ECLAT = rule_ECLAT.reset_index(drop=True)

elapsed_time_eclat1 = elapsed_time_eclat1 + time.time()
rule_ECLAT

"""## Thuật toán ECLAT (mô phỏng)"""

# Thay thế các giá trị NaN thành các giá trị None
# ------------------------------------------------------------------------------
df2 = df_cleaned.replace(np.nan, None)
df2

# Chuyển Data Fomart từ dạng ngang (Horizontal) sang dạng dọc (Vertical)
# ------------------------------------------------------------------------------
vertical_data={}

for i in range(df2.shape[0]):
  for j in range(10):
    if df2.iloc[i][j] is not None:
      if(vertical_data.get(df2.iloc[i][j])):
        vertical_data[df2.iloc[i][j]].append(str(i))
      else:
        vertical_data[df2.iloc[i][j]] = [str(i)]

# Tần số hỗ trợ (Frequency Support)
# ------------------------------------------------------------------------------
freq_sup = {}
for item in vertical_data:
    freq_sup[item] = len(vertical_data[item])

# Xây dựng một số hàm đơn giản để code ngắn gọn hơn và dễ dàng xử lý các tập phổ biến
#  + Chuyển list thành string
#  + Chuyển string thành list
# ------------------------------------------------------------------------------
def list_to_str(l):
    l_list = list(l)
    l_list.sort()
    return ','.join(l_list)

def str_to_list(s):
    return s.split(',')

# Hàm tìm kiếm các tập phổ biến theo đề xuất của (Yin & Han, 2003)
# ------------------------------------------------------------------------------
start_time = time.time()

new_temp_data = vertical_data.copy()
data_temp = vertical_data.copy()
key_set = set()
item_list = []
frequent_item_set = {}
iteration = 0

for item in new_temp_data:
  if(len(new_temp_data[item]) < (min_support*len(df_cleaned))):
    del data_temp[item]

while True:
  iteration+=1

  temp_temp = new_temp_data.copy()
  item_list = list(temp_temp.keys())
  if len(item_list) == 0:
    break
  new_temp_data = {}

  for i in range(len(item_list)):
    for j in range(i+1, len(item_list)):
      if (iteration == 1) or (len(list(set(str_to_list(item_list[i])).intersection(set(str_to_list(item_list[j])))))==iteration-1):
        key_set = set()
        key_set.update(str_to_list(item_list[i]))
        key_set.update(str_to_list(item_list[j]))
        key_set_str = list_to_str(key_set)
        temp = list(set(temp_temp[item_list[i]])&set(temp_temp[item_list[j]]))
        if len(temp) >= (min_support*len(df_cleaned)):
          new_temp_data[key_set_str] = temp
          freq_sup[key_set_str] = len(new_temp_data[key_set_str])
  data_temp.update(new_temp_data)

frequent_item_set = data_temp
elapsed_time_eclat2 = time.time() - start_time

# Tìm luật kết hợp mạnh từ tập phổ biến tìm được ở trên
# ------------------------------------------------------------------------------
start_time = time.time()

frequent_item_set_keys = list(frequent_item_set.keys())
ECLAT_association_rules = []

for frequent_item in frequent_item_set_keys:
  for i in range(1, len(str_to_list(frequent_item))):
    for item_combination in itertools.combinations(str_to_list(frequent_item), i):
      str_item_combination_A = list_to_str(list(item_combination))
      str_item_combination_B = list_to_str(list(set(str_to_list(frequent_item))-set(list(item_combination))))
      conf = freq_sup[frequent_item] / freq_sup[str_item_combination_A]
      if(conf >= min_confidence):
        ECLAT_association_rules.append((str_item_combination_A, str_item_combination_B,
                                         freq_sup[str_item_combination_A]/len(df),
                                         freq_sup[str_item_combination_B]/len(df),
                                         len(frequent_item_set[frequent_item])/len(df),
                                         conf))

elapsed_time_eclat2 = elapsed_time_eclat2 + time.time() - start_time

# Chuyển các luật kết hợp sang DataFrame để quan sát kết quả
# ------------------------------------------------------------------------------
start_time = time.time()

rule_ECLAT_2 = pd.DataFrame(ECLAT_association_rules, columns =['antecedents', 'consequents', 'antecedent_support', 'consequent_support', 'support', 'confidence'])
rule_ECLAT_2['lift'] = rule_ECLAT_2['confidence']/rule_ECLAT_2['consequent_support']
rule_ECLAT_2 = rule_ECLAT_2[rule_ECLAT_2.lift > 1]
rule_ECLAT_2 = rule_ECLAT_2.reset_index(drop=True)

elapsed_time_eclat2 = elapsed_time_eclat2 + time.time() - start_time
rule_ECLAT_2

"""---
## Thuật toán Apriori
"""

# Đưa các giao dịch vào list
# Mỗi list là một itemset chứa các item là mặt hàng mà khách hàng mua
# ------------------------------------------------------------------------------
transaction = []

for _, row in df2.iterrows():
  temp = []
  for column, value in row.iteritems():
    if value is not None:
      i = f"{value}"
      temp.append(i)
  transaction.append(temp)

# TransactionEncoder để chuyển dữ liệu giao dịch thành mảng one-hot encoder boolean
# ------------------------------------------------------------------------------
te = TransactionEncoder()
te_ary = te.fit(transaction).transform(transaction)
df3 = pd.DataFrame(te_ary, columns=te.columns_)
df3

len(transaction)

count = 0
for i in transaction:
  for j in i:
    if j == 'whole milk':
      count += 1
print(count)

df3['whole milk'].value_counts()

# Sử dụng thuật toán Apriori (thư viện mlxtend) để khai pháp tập kết hợp
# ------------------------------------------------------------------------------
start_time = time.time()

df_apriori = apriori(df3, min_support=min_support, use_colnames=True)
df_apriori.itemsets

elapsed_time_apriori = time.time() - start_time

# Sử dụng association_rules để khai phá các luật từ tập phổ biến tìm được
# Ngưỡng yêu cầu là minConf = 0.03 và lift tối thiểu là 1
# ------------------------------------------------------------------------------
start_time = time.time()

rule_APRIORI = association_rules(df_apriori, metric="confidence", min_threshold= min_confidence)
rule_APRIORI = rule_APRIORI[rule_APRIORI.lift > 1]
rule_APRIORI = rule_APRIORI.reset_index(drop=True)

elapsed_time_apriori = elapsed_time_apriori + time.time() - start_time
rule_APRIORI

"""---
# V. Kết quả mô hình
---
Tiến hành so sánh 3 thuật toán:
* Thời gian thực hiện
* Các chỉ số Support, Confidence, Lift...
"""

x = ["ECLAT (pyECLAT)", "ECLAT (Build)", "APRIORI"]
y = [elapsed_time_eclat1, elapsed_time_eclat2, elapsed_time_apriori]

plt.bar(x, y)
plt.xlabel("Algorithm")
plt.ylabel("time in sec")
plt.title("Time Complexity")
plt.show()

x = ["ECLAT (pyECLAT)", "ECLAT (Build)", "APRIORI"]
y = [len(rule_ECLAT), len(rule_ECLAT_2), len(rule_APRIORI)]

plt.bar(x, y)
plt.xlabel("Algorithm")
plt.ylabel("Num of rule")
plt.title("Number of Rule Association")
plt.show()

k = min(len(rule_APRIORI), len(rule_ECLAT), len(rule_ECLAT_2))
x =  [i for i in range(1, k+1)]
y1 = rule_APRIORI["support"].tail(k)
y2 = rule_ECLAT["support"].tail(k)
y3 = rule_ECLAT_2["support"].tail(k)

plt.figure(figsize=(8,4))
plt.plot(x, y1, label='Apriori')
plt.plot(x, y2, label='ECLAT (PyECLAT)')
plt.plot(x, y3, label='ECLAT (Built)')

plt.xlabel('Luật kết hợp')
plt.ylabel('Support value')
plt.title('So sánh support của các thuật toán (sắp xếp tăng dần)')

plt.legend()
plt.show()

k = min(len(rule_APRIORI), len(rule_ECLAT), len(rule_ECLAT_2))
x =  [i for i in range(1, k+1)]
y1 = rule_APRIORI["support"].sort_values().tail(k)
y2 = rule_ECLAT["support"].sort_values().tail(k)
y3 = rule_ECLAT_2["support"].sort_values().tail(k)

plt.figure(figsize=(8,4))
plt.plot(x, y1, label='Apriori')
plt.plot(x, y2, label='ECLAT (PyECLAT)')
plt.plot(x, y3, label='ECLAT (Built)')

plt.xlabel('Luật kết hợp')
plt.ylabel('Support value')
plt.title('So sánh support của các thuật toán (sắp xếp tăng dần)')

plt.legend()
plt.show()

k = min(len(rule_APRIORI), len(rule_ECLAT), len(rule_ECLAT_2))
x =  [i for i in range(1, k+1)]
y1 = rule_APRIORI["lift"].sort_values().tail(k)
y2 = rule_ECLAT["lift"].sort_values().tail(k)
y3 = rule_ECLAT_2["lift"].sort_values().tail(k)

plt.figure(figsize=(8,4))
plt.plot(x, y1, label='Apriori')
plt.plot(x, y2, label='ECLAT (PyECLAT)')
plt.plot(x, y3, label='ECLAT (Built)')

plt.xlabel('Luật kết hợp')
plt.ylabel('Lift value')
plt.title('So sánh lift của các thuật toán')

plt.legend()
plt.show()

k = min(len(rule_APRIORI), len(rule_ECLAT), len(rule_ECLAT_2))
x =  [i for i in range(1, k+1)]
y1 = rule_APRIORI["confidence"].sort_values().tail(k)
y2 = rule_ECLAT["confidence"].sort_values().tail(k)
y3 = rule_ECLAT_2["confidence"].sort_values().tail(k)

plt.figure(figsize=(8,4))
plt.plot(x, y1, label='Apriori')
plt.plot(x, y2, label='ECLAT (PyECLAT)')
plt.plot(x, y3, label='ECLAT (Built)')

plt.xlabel('Luật kết hợp')
plt.ylabel('Confidence value')
plt.title('So sánh confidence của các thuật toán')

plt.legend()
plt.show()

fig = px.scatter(rule_ECLAT, x = 'support', y = 'lift',
                 color = 'confidence', color_continuous_scale=px.colors.sequential.Viridis,
                 title = 'Mối quan hệ giữa support, confidence và lift trong thuật toán ECLAT (pyECLAT)')
fig.show()

fig = px.scatter(rule_ECLAT_2, x = 'support', y = 'lift',
                 color = 'confidence', color_continuous_scale=px.colors.sequential.Viridis,
                 title = 'Mối quan hệ giữa support, confidence và lift trong thuật toán ECLAT (built)')
fig.show()

fig = px.scatter(rule_APRIORI, x = 'support', y = 'lift',
                 color = 'confidence', color_continuous_scale=px.colors.sequential.Viridis,
                 title = 'Mối quan hệ giữa support, confidence và lift trong thuật toán Apriori')
fig.show()